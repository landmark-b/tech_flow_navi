{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a87317f",
   "metadata": {},
   "source": [
    "# はじめに\n",
    "- TechFlow Navigatorは特定分野の社会・技術動向を提示してくれるAIエージェント\n",
    "\n",
    "## 注意\n",
    "- OPENAIのAPIを使う場合Emdebbingしているため費用が発生する。\n",
    "    - ColaboratoryのSecret KeyとしてOPENAI_API_KEYを指定しなければ無料のSentenceTransformerが利用される\n",
    "\n",
    "\n",
    "## 全体フロー\n",
    "--- 1. 論文取得モジュール ---  \n",
    "API経由でarXivから特定テーマの情報収集  \n",
    "\n",
    "--- 2. Embedding生成モジュール --- \n",
    "Embeddingを生成しベクトルDBへ投入  \n",
    "\n",
    "--- 3. 系譜推定モジュール ---    \n",
    "\n",
    "--- 4. LLMによる解説生成モジュール (Switchable) ---  \n",
    "\n",
    "--- 5. ユーティリティ: 結果表示  ---  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e555ee",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pip install feedparser\n",
    "!pip install openai pandas tqdm\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea396531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "try:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    pass # OPENAI_API_KEY未設定時はMock版(定型文の解説を出力)として動作\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import feedparser\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58505ff9",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab68169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 論文取得モジュール ---\n",
    "def fetch_arxiv_papers(query=\"scaling law\", max_results=30):\n",
    "    \"\"\"\n",
    "    arXivのAPIを使用して論文情報を取得する。\n",
    "\n",
    "    Args:\n",
    "        query (str): 検索クエリ（例: \"LLM scaling law\"）。\n",
    "        max_results (int): 取得する最大論文数。\n",
    "\n",
    "    Returns:\n",
    "        list: 取得した論文データ（ID、タイトル、サマリー、日付など）のリスト。\n",
    "              取得失敗時は空のリストを返す。\n",
    "    \"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    \n",
    "    # 検索パラメータ\n",
    "    params = {\n",
    "        \"search_query\": f\"all:{query}\",\n",
    "        \"start\": 0,\n",
    "        \"max_results\": max_results,\n",
    "        # \"sortBy\": \"submittedDate\",\n",
    "        # \"sortOrder\": \"descending\" # 新しいものが先に取得されることが多い\n",
    "    }\n",
    "\n",
    "    print(f\"Fetching papers for query: '{query}'...\")\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: API returned status {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    feed = feedparser.parse(response.text)\n",
    "    \n",
    "    papers = []\n",
    "    for entry in feed.entries:\n",
    "        papers.append({\n",
    "            \"id\": entry.get(\"id\"),\n",
    "            \"title\": entry.get(\"title\").replace(\"\\n\", \" \"),\n",
    "            \"summary\": entry.get(\"summary\").replace(\"\\n\", \" \"),\n",
    "            \"authors\": [a.name for a in entry.get(\"authors\", [])],\n",
    "            \"published\": entry.get(\"published\"),\n",
    "            \"updated\": entry.get(\"updated\")\n",
    "        })\n",
    "    \n",
    "    print(f\"Fetched {len(papers)} papers.\")\n",
    "    return papers\n",
    "\n",
    "# --- 2. Embedding生成モジュール ---\n",
    "def generate_embeddings(papers):\n",
    "    \"\"\"\n",
    "    論文のTitle/AbstractからEmbeddingを生成し、リストに追加する。\n",
    "\n",
    "    Args:\n",
    "        papers (list): 論文データ（'title', 'summary'を含む）のリスト。\n",
    "\n",
    "    Returns:\n",
    "        list: 'embedding'キーが付加された論文データのリスト。\n",
    "    \"\"\"\n",
    "    print(\"Generating embeddings (this may take a moment)...\")\n",
    "    \n",
    "    # タイトルとサマリーを結合して特徴量とする\n",
    "    texts = [f\"{p['title']} {p['summary']}\" for p in papers]\n",
    "    \n",
    "    # モデル読み込み（MVPでは軽量な公開モデルを使用）\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    embeddings = model.encode(texts, batch_size=8, show_progress_bar=True)\n",
    "    \n",
    "    # Embeddingをリスト形式で各論文データに追加\n",
    "    for p, emb in zip(papers, embeddings):\n",
    "        p[\"embedding\"] = emb.tolist()\n",
    "        \n",
    "    return papers\n",
    "\n",
    "# --- 3. 系譜推定モジュール ---\n",
    "def parse_arxiv_date(date_str: str) -> datetime:\n",
    "    \"\"\"\n",
    "    arXivの日付文字列をdatetimeオブジェクトに変換する。\n",
    "\n",
    "    Args:\n",
    "        date_str (str): arXivの日付文字列（例: '2020-01-23T00:00:00Z'）。\n",
    "\n",
    "    Returns:\n",
    "        datetime: 変換されたdatetimeオブジェクト。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    except ValueError:\n",
    "        return datetime.now() # パース失敗時は現在時刻を返すなどの対応\n",
    "\n",
    "def estimate_lineage(papers, similarity_threshold=0.5):\n",
    "    \"\"\"\n",
    "    時系列順に論文をソートし、過去の論文とのEmbedding類似度に基づいて親子関係（系譜）を推定する。\n",
    "\n",
    "    Args:\n",
    "        papers (list): 'embedding'と'published'キーを含む論文データのリスト。\n",
    "        similarity_threshold (float): 親とみなす類似度の閾値（0.0～1.0）。\n",
    "\n",
    "    Returns:\n",
    "        list: 親子関係情報 ('parent_id', 'parent_title', 'relation_score') が付与され、\n",
    "              時系列順にソートされた論文リスト。\n",
    "    \"\"\"\n",
    "    print(\"Estimating lineage...\")\n",
    "    \n",
    "    # 1. 時系列ソート\n",
    "    for p in papers:\n",
    "        p['_dt'] = parse_arxiv_date(p['published'])\n",
    "    sorted_papers = sorted(papers, key=lambda x: x['_dt'])\n",
    "    \n",
    "    valid_papers = [p for p in sorted_papers if \"embedding\" in p]\n",
    "    if not valid_papers:\n",
    "        return []\n",
    "        \n",
    "    embeddings_matrix = np.array([p['embedding'] for p in valid_papers])\n",
    "    \n",
    "    # 2. 類似度行列の一括計算\n",
    "    sim_matrix = cosine_similarity(embeddings_matrix)\n",
    "    \n",
    "    # 3. 親子関係の決定ロジック\n",
    "    for i, current_paper in enumerate(valid_papers):\n",
    "        current_paper['parent_id'] = None\n",
    "        current_paper['parent_title'] = None\n",
    "        current_paper['relation_score'] = 0.0\n",
    "        \n",
    "        if i == 0:\n",
    "            continue\n",
    "            \n",
    "        # 過去の論文との類似度を取得\n",
    "        past_sims = sim_matrix[i, :i]\n",
    "        candidate_indices = np.where(past_sims >= similarity_threshold)[0]\n",
    "        \n",
    "        if len(candidate_indices) > 0:\n",
    "            # 最も類似度が高いものを親とする\n",
    "            best_idx = candidate_indices[np.argmax(past_sims[candidate_indices])]\n",
    "            \n",
    "            parent = valid_papers[best_idx]\n",
    "            current_paper['parent_id'] = parent.get('id')\n",
    "            current_paper['parent_title'] = parent.get('title')\n",
    "            current_paper['relation_score'] = float(past_sims[best_idx])\n",
    "            \n",
    "    # 一時的な日付オブジェクトを削除\n",
    "    for p in valid_papers:\n",
    "        del p['_dt']\n",
    "        \n",
    "    return valid_papers\n",
    "\n",
    "\n",
    "# --- 4. LLMによる解説生成モジュール (Switchable) ---\n",
    "def _mock_explanation(score):\n",
    "    \"\"\"APIキーがない場合のダミー応答\"\"\"\n",
    "    if score >= 0.7:\n",
    "        return \"パラダイムシフトとなる新構造を提案し、大幅な効率化を実現(Mock)\"\n",
    "    elif score >= 0.6:\n",
    "        return \"先行研究の課題であった計算コストを、新手法により削減(Mock)\"\n",
    "    else:\n",
    "        return \"類似の課題設定に対し、異なるデータセットを用いて検証(Mock)\"\n",
    "\n",
    "def _real_explanation_openai(parent_title, child_title, api_key):\n",
    "    \"\"\"OpenAI APIを使って解説を生成する\"\"\"\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    あなたは技術リサーチャーです。以下の2つの論文の関係性を分析してください。\n",
    "    \n",
    "    [親論文]: {parent_title}\n",
    "    [子論文]: {child_title}\n",
    "    \n",
    "    指示: 親から子へ、技術的に何が進化したか、またはどう応用されたかを30文字程度の日本語で要約してください。\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=LLM_MODEL, \n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=60,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error generation explanation: {str(e)}\"\n",
    "\n",
    "def generate_relation_explanation_dispatcher(parent_title, child_title, score):\n",
    "    \"\"\"\n",
    "    環境変数を見てMockか本番かを自動で切り替えるディスパッチャー\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    if api_key:\n",
    "        return _real_explanation_openai(parent_title, child_title, api_key)\n",
    "    else:\n",
    "        return _mock_explanation(score)\n",
    "\n",
    "def process_llm_explanation(lineage_papers):\n",
    "    \"\"\"\n",
    "    系譜データに対して解説文を生成・付与するメイン関数\n",
    "\n",
    "    Args:\n",
    "        lineage_papers (list): 系譜推定後の論文データのリスト。\n",
    "\n",
    "    Returns:\n",
    "        list: LLMによる解説 ('relation_description_by_llm') が付加された論文リスト。\n",
    "\n",
    "    \"\"\"\n",
    "    # 実行モードの表示\n",
    "    if os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"\\nGenerating explanations using OpenAI API...\")\n",
    "    else:\n",
    "        print(\"\\nGenerating explanations using Mock (No API Key found)...\")\n",
    "    \n",
    "    papers_with_parent = [p for p in lineage_papers if p.get('parent_id')]\n",
    "    \n",
    "    for i, p in enumerate(papers_with_parent):\n",
    "        # 進捗表示もあると親切\n",
    "        print(f\"  - Explaining pair {i+1}/{len(papers_with_parent)}\")\n",
    "        \n",
    "        # ディスパッチャー経由で呼び出すように変更\n",
    "        explanation = generate_relation_explanation_dispatcher(\n",
    "            p['parent_title'], \n",
    "            p['title'], \n",
    "            p['relation_score']\n",
    "        )\n",
    "        p['relation_description_by_llm'] = explanation\n",
    "        \n",
    "    return lineage_papers\n",
    "\n",
    "\n",
    "# --- ユーティリティ: 結果表示  ---\n",
    "def print_lineage_summary(papers: list):\n",
    "    \"\"\"\n",
    "    推定された系譜とLLMによる解説をコンソールに出力する。\n",
    "\n",
    "    Args:\n",
    "        papers (list): 系譜情報とLLM解説が付与された論文データのリスト。\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Lineage Summary (with LLM Explanations) ===\")\n",
    "    \n",
    "    # 系譜のルートノードを識別し、ツリー構造に近い表示を試みる\n",
    "    for p in papers:\n",
    "        if p.get('parent_id'):\n",
    "            # 子ノード\n",
    "            date_str = p['published'][:10]\n",
    "            title = p['title'][:50]\n",
    "            \n",
    "            # 親の情報\n",
    "            score = p['relation_score']\n",
    "            p_title = p['parent_title'][:40]\n",
    "            llm_desc = p.get('relation_description_by_llm', '未生成')\n",
    "\n",
    "            print(\"--------------------------------------------------\")\n",
    "            print(f\"[{date_str}] {title}...\")\n",
    "            print(f\"   ↑ ({score:.2f}) {llm_desc}\")\n",
    "            print(f\"   └ From: {p_title}...\")\n",
    "        else:\n",
    "            # ルートノード\n",
    "            date_str = p['published'][:10]\n",
    "            title = p['title'][:50]\n",
    "            print(f\"\\n--- ROOT / Independent ---\")\n",
    "            print(f\"[{date_str}] {title}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2349c8d",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1009487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- メイン実行ブロック ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 設定\n",
    "    LLM_MODEL = \"gpt-4o-mini\"\n",
    "    QUERY = \"scaling law\"\n",
    "    MAX_RESULTS = 50\n",
    "    SIMILARITY_THRESHOLD = 0.6\n",
    "    OUTPUT_FILE = \"arxiv_lineage_result.json\"\n",
    "\n",
    "    # 1. 取得\n",
    "    papers = fetch_arxiv_papers(query=QUERY, max_results=MAX_RESULTS)\n",
    "\n",
    "    if papers:\n",
    "        # 2. Embedding生成\n",
    "        papers = generate_embeddings(papers)\n",
    "\n",
    "        # 3. 系譜推定\n",
    "        lineage_papers = estimate_lineage(papers, similarity_threshold=SIMILARITY_THRESHOLD)\n",
    "\n",
    "        # 4. コンソール表示\n",
    "        print_lineage_summary(lineage_papers)\n",
    "\n",
    "        # 5. LLMによる関係性の言語化 (New!)\n",
    "        lineage_papers = process_llm_explanation(lineage_papers)\n",
    "\n",
    "        # 6. JSON保存\n",
    "        with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            saveable_papers = [{k: v for k, v in p.items() if k != 'embedding'} for p in lineage_papers]\n",
    "    \n",
    "            # データを保存\n",
    "            json.dump(saveable_papers, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nSaved results to {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"No papers found.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
